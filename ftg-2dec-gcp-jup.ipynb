{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils, plot_model, to_categorical\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(dim_p, n_units):\n",
    "\t# define training encoder\n",
    "\tenc_in_o = Input(shape=(None, 1))\n",
    "\tenc_in_q = Input(shape=(None, 1))\n",
    "\tenc_in_p = Input(shape=(None, dim_p))\n",
    "\tencoder_inputs = concatenate([enc_in_o, enc_in_q, enc_in_p])\n",
    "\tencoder = LSTM(n_units, return_state=True)\n",
    "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\tencoder_states = [state_h, state_c]\n",
    "\t\n",
    "\t# define training decoder 1\n",
    "\tdec_in_o = Input(shape=(None, 1))\n",
    "\tdec_in_q = Input(shape=(None, 1))\n",
    "\tdec_in_p = Input(shape=(None, dim_p))\n",
    "\tdecoder_inputs = concatenate([dec_in_o, dec_in_q, dec_in_p])\n",
    "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\tdec_dense_o = Dense(1, activation='relu', name='tr_out_o')\n",
    "\tdec_dense_q = Dense(1, activation='sigmoid', name='tr_out_q')\n",
    "\tdec_dense_p = Dense(dim_p, activation='softmax', name='tr_out_p')\n",
    "\t#out_o = Dense(1, activation='relu', name='tr_out_o')(decoder_outputs)#act relu\n",
    "\t#out_q = Dense(1, activation='sigmoid', name='tr_out_q')(decoder_outputs)\n",
    "\t#out_p = Dense(dim_p, activation='softmax', name='tr_out_p')(decoder_outputs)\n",
    "\tout_o = dec_dense_o(decoder_outputs)\n",
    "\tout_q = dec_dense_q(decoder_outputs)\n",
    "\tout_p = dec_dense_p(decoder_outputs)\n",
    "\n",
    "\t# define training decoder 2\n",
    "\tdec_in_o_2 = Input(shape=(None, 1))\n",
    "\tdec_in_q_2 = Input(shape=(None, 1))\n",
    "\tdec_in_p_2 = Input(shape=(None, dim_p))\n",
    "\tdecoder_inputs_2 = concatenate([dec_in_o_2, dec_in_q_2, dec_in_p_2])\n",
    "\tdecoder_lstm_2 = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs_2, _, _ = decoder_lstm_2(decoder_inputs_2, initial_state=encoder_states)\n",
    "\tdec_dense_o_2 = Dense(1, activation='relu', name='tr_out_o_2')\n",
    "\tdec_dense_q_2 = Dense(1, activation='sigmoid', name='tr_out_q_2')\n",
    "\tdec_dense_p_2 = Dense(dim_p, activation='softmax', name='tr_out_p_2')\n",
    "\t#out_o = Dense(1, activation='relu', name='tr_out_o')(decoder_outputs)#act relu\n",
    "\t#out_q = Dense(1, activation='sigmoid', name='tr_out_q')(decoder_outputs)\n",
    "\t#out_p = Dense(dim_p, activation='softmax', name='tr_out_p')(decoder_outputs)\n",
    "\tout_o_2 = dec_dense_o_2(decoder_outputs_2)\n",
    "\tout_q_2 = dec_dense_q_2(decoder_outputs_2)\n",
    "\tout_p_2 = dec_dense_p_2(decoder_outputs_2)\n",
    "\t\n",
    "\tmodel = Model([enc_in_o, enc_in_q, enc_in_p, dec_in_o, dec_in_q, dec_in_p], [out_o, out_q, out_p])\n",
    "\tmodel_2 = Model([enc_in_o, enc_in_q, enc_in_p, dec_in_o_2, dec_in_q_2, dec_in_p_2], [out_o_2, out_q_2, out_p_2])\n",
    "\t\n",
    "\t# define inference encoder\n",
    "\tencoder_model = Model([enc_in_o, enc_in_q, enc_in_p], encoder_states)\n",
    "\t\n",
    "\t# define inference decoder 1\n",
    "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\tdecoder_states = [state_h, state_c]\n",
    "\t#out_o = TimeDistributed(Dense(1, activation='relu'))(decoder_outputs)#act relu\n",
    "\t#out_q = TimeDistributed(Dense(1, activation='sigmoid'))(decoder_outputs)\n",
    "\t#out_p = TimeDistributed(Dense(dim_p, activation='softmax'))(decoder_outputs)\n",
    "\tout_o = dec_dense_o(decoder_outputs)\n",
    "\tout_q = dec_dense_q(decoder_outputs)\n",
    "\tout_p = dec_dense_p(decoder_outputs)\n",
    "\n",
    "\t# define inference decoder 2\n",
    "\tdecoder_state_input_h_2 = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c_2 = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs_2 = [decoder_state_input_h, decoder_state_input_c_2]\n",
    "\tdecoder_outputs_2, state_h_2, state_c_2 = decoder_lstm_2(decoder_inputs_2, initial_state=decoder_states_inputs_2)\n",
    "\tdecoder_states_2 = [state_h_2, state_c_2]\n",
    "\t#out_o = TimeDistributed(Dense(1, activation='relu'))(decoder_outputs)#act relu\n",
    "\t#out_q = TimeDistributed(Dense(1, activation='sigmoid'))(decoder_outputs)\n",
    "\t#out_p = TimeDistributed(Dense(dim_p, activation='softmax'))(decoder_outputs)\n",
    "\tout_o_2 = dec_dense_o_2(decoder_outputs_2)\n",
    "\tout_q_2 = dec_dense_q_2(decoder_outputs_2)\n",
    "\tout_p_2 = dec_dense_p_2(decoder_outputs_2)\n",
    "\n",
    "\tdecoder_model = Model([dec_in_o, dec_in_q, dec_in_p] + decoder_states_inputs, [out_o, out_q, out_p] + decoder_states)\n",
    "\tdecoder_model_2 = Model([dec_in_o_2, dec_in_q_2, dec_in_p_2] + decoder_states_inputs_2, [out_o_2, out_q_2, out_p_2] + decoder_states_2)\n",
    "\n",
    "\t# return all models\n",
    "\treturn model, model_2, encoder_model, decoder_model, decoder_model_2\n",
    "\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, src_o, src_q, src_p, n_steps, cardinality):\n",
    "\t# encode\n",
    "\tstate = infenc.predict([src_o, src_q, src_p])\n",
    "\t# start of sequence input\n",
    "\ttarget_o = np.array([-1]).reshape(1, 1, 1)\n",
    "\ttarget_q = np.array([-1]).reshape(1, 1, 1)\n",
    "\t#target_p = 0\n",
    "\ttarget_p = np.array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\t#print(target_o.shape)\n",
    "\t\t#print(target_q.shape)\n",
    "\t\t#print(target_p.shape)\n",
    "\t\t#print(state[0].shape)\n",
    "\t\to, q, p, h, c = infdec.predict([target_o, target_q, target_p] + state)\n",
    "\t\t#print(a)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(o[0,0,:])\n",
    "\t\toutput.append(q[0,0,:])\n",
    "\t\toutput.append(p[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_o = o\n",
    "\t\ttarget_q = q\n",
    "\t\ttarget_p = p\n",
    "\treturn np.array(output)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [np.argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "#create list with window length sequences of list a data\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "#generate train inputs and outputs while one hot encoding pitch and padding for seq2seq\n",
    "def generatorex(features1, features2, features3, seq_length, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels# \n",
    "    batch_features1 = np.zeros((batch_size, seq_length, 1))\n",
    "    batch_features2 = np.zeros((batch_size, seq_length, 1))\n",
    "    batch_features3 = np.zeros((batch_size, seq_length, 128))\n",
    "    batch_feat_pad1 = np.zeros((batch_size, seq_length, 1))\n",
    "    batch_feat_pad2 = np.zeros((batch_size, seq_length, 1))\n",
    "    batch_feat_pad3 = np.zeros((batch_size, seq_length, 128))\n",
    "    i = 0\n",
    "    while True:\n",
    "        for b in range(batch_size):\n",
    "            batch_features1[b] = features1[i]\n",
    "            batch_features2[b] = features2[i]\n",
    "            batch_features3[b] = to_categorical(features3[i], num_classes=128)\n",
    "            batch_feat_pad1[b] = np.append([-1], features1[i][:-1]).reshape(seq_length, 1)\n",
    "            batch_feat_pad2[b] = np.append([-1], features2[i][:-1]).reshape(seq_length, 1)\n",
    "            batch_feat_pad3[b] = to_categorical(np.append([0], features3[i][:-1]).reshape(seq_length, 1), num_classes=128)\n",
    "            i += 1\n",
    "            if (i == len(features1)):\n",
    "                i=0\n",
    "        #print(batch_features, batch_labels)\n",
    "        yield [batch_features1, batch_features2, batch_features3, batch_feat_pad1, batch_feat_pad2, batch_feat_pad3], [batch_features1, batch_features2, batch_features3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "stream_list = []\n",
    "stream_list_2 = []\n",
    "\n",
    "for path, subdirectories, files in os.walk('/kaggle/input/data-rock/kag'):\n",
    "    for name in files:\n",
    "        with open(os.path.join(path, name), 'r') as f: \n",
    "            reader = csv.reader(f)\n",
    "            sub_list = [list(map(float,rec)) for rec in csv.reader(f, delimiter=',')]\n",
    "            stream_list = stream_list + sub_list\n",
    "            \n",
    "for path, subdirectories, files in os.walk('/kaggle/input/data-jazz'):\n",
    "    for name in files:\n",
    "        with open(os.path.join(path, name), 'r') as f: \n",
    "            reader = csv.reader(f)\n",
    "            sub_list = [list(map(float,rec)) for rec in csv.reader(f, delimiter=',')]\n",
    "            stream_list_2 = stream_list_2 + sub_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create seperate data structures for each variable (offset, quarterlength, pitch)\n",
    "#normalise offset and quarterlength\n",
    "offs = []\n",
    "qlngth = []\n",
    "ptch = []\n",
    "\n",
    "offs_2 = []\n",
    "qlngth_2 = []\n",
    "ptch_2 = []\n",
    "\n",
    "offsb = max(element[0] for element in stream_list if element[0]<=600.0)\n",
    "qlngthb = max(element[1] for element in stream_list if element[1]<=50.0)\n",
    "#ptchb = 127.0\n",
    "offsb_2 = max(element[0] for element in stream_list_2 if element[0]<=600.0)\n",
    "qlngthb_2 = max(element[1] for element in stream_list_2 if element[1]<=50.0)\n",
    "\n",
    "for row in stream_list:\n",
    "    if (row[0] <= 600.0 and row[1] <= 50.0):\n",
    "        offs.append(row[0]/offsb)\n",
    "        qlngth.append(row[1]/qlngthb)\n",
    "        ptch.append(row[2])\n",
    "        \n",
    "for row in stream_list_2:\n",
    "    if (row[0] <= 600.0 and row[1] <= 50.0):\n",
    "        offs_2.append(row[0]/offsb_2)\n",
    "        qlngth_2.append(row[1]/qlngthb_2)\n",
    "        ptch_2.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  [465336, 322337]\n"
     ]
    }
   ],
   "source": [
    "#divide the sets in sequences of specific length \n",
    "dtlngth=[len(offs), len(offs_2)]\n",
    "seq_length = 20#100 groups of 3\n",
    "\n",
    "dataX1_o = rolling_window(np.asarray(offs), seq_length)\n",
    "dataX1_q = rolling_window(np.asarray(qlngth), seq_length)\n",
    "dataX1_p = rolling_window(np.asarray(ptch), seq_length)\n",
    "\n",
    "dataX1_o_2 = rolling_window(np.asarray(offs_2), seq_length)\n",
    "dataX1_q_2 = rolling_window(np.asarray(qlngth_2), seq_length)\n",
    "dataX1_p_2 = rolling_window(np.asarray(ptch_2), seq_length)\n",
    "\n",
    "n_patterns = [len(dataX1_p), len(dataX1_p_2)]\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape inputs to be [samples, time steps, features]\n",
    "dataX1_o = np.reshape(dataX1_o, (dtlngth[0] - seq_length + 1, seq_length, 1))\n",
    "dataX1_q = np.reshape(dataX1_q, (dtlngth[0] - seq_length + 1, seq_length, 1))\n",
    "dataX1_p = np.reshape(dataX1_p, (dtlngth[0] - seq_length + 1, seq_length, 1))\n",
    "\n",
    "dataX1_o_2 = np.reshape(dataX1_o_2, (dtlngth[1] - seq_length + 1, seq_length, 1))\n",
    "dataX1_q_2 = np.reshape(dataX1_q_2, (dtlngth[1] - seq_length + 1, seq_length, 1))\n",
    "dataX1_p_2 = np.reshape(dataX1_p_2, (dtlngth[1] - seq_length + 1, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Patterns:  [46533, 32233]\n"
     ]
    }
   ],
   "source": [
    "#divide data in train and validation sets\n",
    "split_i = [n_patterns[0]*10 // 100, n_patterns[1]*10 // 100]\n",
    "\n",
    "dataX1_o_v = dataX1_o[-split_i[0]:]\n",
    "dataX1_o = dataX1_o[:-split_i[0]]\n",
    "\n",
    "dataX1_q_v = dataX1_q[-split_i[0]:]\n",
    "dataX1_q = dataX1_q[:-split_i[0]]\n",
    "\n",
    "dataX1_p_v = dataX1_p[-split_i[0]:]\n",
    "dataX1_p = dataX1_p[:-split_i[0]]\n",
    "\n",
    "dataX1_o_v_2 = dataX1_o_2[-split_i[1]:]\n",
    "dataX1_o_2 = dataX1_o_2[:-split_i[1]]\n",
    "\n",
    "dataX1_q_v_2 = dataX1_q_2[-split_i[1]:]\n",
    "dataX1_q_2 = dataX1_q_2[:-split_i[1]]\n",
    "\n",
    "dataX1_p_v_2 = dataX1_p_2[-split_i[1]:]\n",
    "dataX1_p_2 = dataX1_p_2[:-split_i[1]]\n",
    "\n",
    "print (\"Validation Patterns: \", split_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure problem\n",
    "n_features = 127+1\n",
    "n_steps_out = seq_length\n",
    "# define models\n",
    "train, train_2, infenc, infdec, infdec_2 = define_models(n_features, 256)\n",
    "train.compile(optimizer='adam', loss={'tr_out_o': 'mse', 'tr_out_q': 'mse', 'tr_out_p': 'categorical_crossentropy'},\n",
    " metrics={'tr_out_o': 'mean_squared_error', 'tr_out_q': 'mean_squared_error', 'tr_out_p': 'accuracy'})\n",
    "train_2.compile(optimizer='adam', loss={'tr_out_o_2': 'mse', 'tr_out_q_2': 'mse', 'tr_out_p_2': 'categorical_crossentropy'},\n",
    " metrics={'tr_out_o_2': 'mean_squared_error', 'tr_out_q_2': 'mean_squared_error', 'tr_out_p_2': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1_a\n",
      "77/77 [==============================] - 239s 3s/step - loss: 0.5454 - tr_out_o_loss: 0.0026 - tr_out_q_loss: 9.0506e-04 - tr_out_p_loss: 0.5419 - tr_out_o_mean_squared_error: 0.0026 - tr_out_q_mean_squared_error: 9.0506e-04 - tr_out_p_accuracy: 0.8285\n",
      "Epoch 1_b\n",
      "53/53 [==============================] - 164s 3s/step - loss: 0.6340 - tr_out_o_2_loss: 0.0027 - tr_out_q_2_loss: 7.5093e-04 - tr_out_p_2_loss: 0.6305 - tr_out_o_2_mean_squared_error: 0.0027 - tr_out_q_2_mean_squared_error: 7.5093e-04 - tr_out_p_2_accuracy: 0.8030\n",
      "Epoch 2_a\n",
      "77/77 [==============================] - 238s 3s/step - loss: 0.5300 - tr_out_o_loss: 0.0025 - tr_out_q_loss: 9.0285e-04 - tr_out_p_loss: 0.5266 - tr_out_o_mean_squared_error: 0.0025 - tr_out_q_mean_squared_error: 9.0285e-04 - tr_out_p_accuracy: 0.8331\n",
      "Epoch 2_b\n",
      "53/53 [==============================] - 165s 3s/step - loss: 0.6151 - tr_out_o_2_loss: 0.0026 - tr_out_q_2_loss: 7.4973e-04 - tr_out_p_2_loss: 0.6118 - tr_out_o_2_mean_squared_error: 0.0026 - tr_out_q_2_mean_squared_error: 7.4973e-04 - tr_out_p_2_accuracy: 0.8095\n",
      "Epoch 3_a\n",
      "77/77 [==============================] - 238s 3s/step - loss: 0.5132 - tr_out_o_loss: 0.0025 - tr_out_q_loss: 9.0386e-04 - tr_out_p_loss: 0.5099 - tr_out_o_mean_squared_error: 0.0025 - tr_out_q_mean_squared_error: 9.0386e-04 - tr_out_p_accuracy: 0.8391\n",
      "Epoch 3_b\n",
      "53/53 [==============================] - 167s 3s/step - loss: 0.5493 - tr_out_o_2_loss: 0.0025 - tr_out_q_2_loss: 7.5132e-04 - tr_out_p_2_loss: 0.5461 - tr_out_o_2_mean_squared_error: 0.0025 - tr_out_q_2_mean_squared_error: 7.5132e-04 - tr_out_p_2_accuracy: 0.8317\n",
      "Epoch 4_a\n",
      "77/77 [==============================] - 251s 3s/step - loss: 0.4638 - tr_out_o_loss: 0.0023 - tr_out_q_loss: 8.9763e-04 - tr_out_p_loss: 0.4606 - tr_out_o_mean_squared_error: 0.0023 - tr_out_q_mean_squared_error: 8.9763e-04 - tr_out_p_accuracy: 0.8569\n",
      "Epoch 4_b\n",
      "53/53 [==============================] - 167s 3s/step - loss: 0.5320 - tr_out_o_2_loss: 0.0024 - tr_out_q_2_loss: 7.5068e-04 - tr_out_p_2_loss: 0.5288 - tr_out_o_2_mean_squared_error: 0.0024 - tr_out_q_2_mean_squared_error: 7.5068e-04 - tr_out_p_2_accuracy: 0.8371\n",
      "Epoch 5_a\n",
      "77/77 [==============================] - 240s 3s/step - loss: 0.4371 - tr_out_o_loss: 0.0023 - tr_out_q_loss: 8.9879e-04 - tr_out_p_loss: 0.4339 - tr_out_o_mean_squared_error: 0.0023 - tr_out_q_mean_squared_error: 8.9879e-04 - tr_out_p_accuracy: 0.8649\n",
      "Epoch 5_b\n",
      "53/53 [==============================] - 165s 3s/step - loss: 0.5166 - tr_out_o_2_loss: 0.0024 - tr_out_q_2_loss: 7.4990e-04 - tr_out_p_2_loss: 0.5134 - tr_out_o_2_mean_squared_error: 0.0024 - tr_out_q_2_mean_squared_error: 7.4990e-04 - tr_out_p_2_accuracy: 0.8417\n",
      "Epoch 6_a\n",
      "77/77 [==============================] - 239s 3s/step - loss: 0.4240 - tr_out_o_loss: 0.0022 - tr_out_q_loss: 8.9928e-04 - tr_out_p_loss: 0.4208 - tr_out_o_mean_squared_error: 0.0022 - tr_out_q_mean_squared_error: 8.9928e-04 - tr_out_p_accuracy: 0.8695\n",
      "Epoch 6_b\n",
      "53/53 [==============================] - 165s 3s/step - loss: 0.4755 - tr_out_o_2_loss: 0.0023 - tr_out_q_2_loss: 7.4976e-04 - tr_out_p_2_loss: 0.4724 - tr_out_o_2_mean_squared_error: 0.0023 - tr_out_q_2_mean_squared_error: 7.4976e-04 - tr_out_p_2_accuracy: 0.8556\n",
      "Epoch 7_a\n",
      "77/77 [==============================] - 239s 3s/step - loss: 0.4155 - tr_out_o_loss: 0.0023 - tr_out_q_loss: 8.9687e-04 - tr_out_p_loss: 0.4123 - tr_out_o_mean_squared_error: 0.0023 - tr_out_q_mean_squared_error: 8.9687e-04 - tr_out_p_accuracy: 0.8707\n",
      "Epoch 7_b\n",
      "53/53 [==============================] - 167s 3s/step - loss: 0.4533 - tr_out_o_2_loss: 0.0023 - tr_out_q_2_loss: 7.4808e-04 - tr_out_p_2_loss: 0.4502 - tr_out_o_2_mean_squared_error: 0.0023 - tr_out_q_2_mean_squared_error: 7.4808e-04 - tr_out_p_2_accuracy: 0.8634\n",
      "Epoch 8_a\n",
      "77/77 [==============================] - 244s 3s/step - loss: 0.3864 - tr_out_o_loss: 0.0021 - tr_out_q_loss: 8.9766e-04 - tr_out_p_loss: 0.3834 - tr_out_o_mean_squared_error: 0.0021 - tr_out_q_mean_squared_error: 8.9766e-04 - tr_out_p_accuracy: 0.8814\n",
      "Epoch 8_b\n",
      "53/53 [==============================] - 165s 3s/step - loss: 0.4435 - tr_out_o_2_loss: 0.0023 - tr_out_q_2_loss: 7.4746e-04 - tr_out_p_2_loss: 0.4405 - tr_out_o_2_mean_squared_error: 0.0023 - tr_out_q_2_mean_squared_error: 7.4746e-04 - tr_out_p_2_accuracy: 0.8658\n",
      "Epoch 9_a\n",
      "77/77 [==============================] - 240s 3s/step - loss: 0.3657 - tr_out_o_loss: 0.0021 - tr_out_q_loss: 8.9938e-04 - tr_out_p_loss: 0.3627 - tr_out_o_mean_squared_error: 0.0021 - tr_out_q_mean_squared_error: 8.9938e-04 - tr_out_p_accuracy: 0.8882\n",
      "Epoch 9_b\n",
      "53/53 [==============================] - 164s 3s/step - loss: 0.4382 - tr_out_o_2_loss: 0.0023 - tr_out_q_2_loss: 7.4920e-04 - tr_out_p_2_loss: 0.4351 - tr_out_o_2_mean_squared_error: 0.0023 - tr_out_q_2_mean_squared_error: 7.4920e-04 - tr_out_p_2_accuracy: 0.8667\n",
      "Epoch 10_a\n",
      "77/77 [==============================] - 240s 3s/step - loss: 0.3495 - tr_out_o_loss: 0.0021 - tr_out_q_loss: 8.9498e-04 - tr_out_p_loss: 0.3465 - tr_out_o_mean_squared_error: 0.0021 - tr_out_q_mean_squared_error: 8.9498e-04 - tr_out_p_accuracy: 0.8933\n",
      "Epoch 10_b\n",
      "53/53 [==============================] - 165s 3s/step - loss: 0.3951 - tr_out_o_2_loss: 0.0023 - tr_out_q_2_loss: 7.4685e-04 - tr_out_p_2_loss: 0.3921 - tr_out_o_2_mean_squared_error: 0.0023 - tr_out_q_2_mean_squared_error: 7.4685e-04 - tr_out_p_2_accuracy: 0.8820\n",
      "Epoch 11_a\n",
      "77/77 [==============================] - 240s 3s/step - loss: 0.3371 - tr_out_o_loss: 0.0021 - tr_out_q_loss: 8.9003e-04 - tr_out_p_loss: 0.3341 - tr_out_o_mean_squared_error: 0.0021 - tr_out_q_mean_squared_error: 8.9003e-04 - tr_out_p_accuracy: 0.8977\n",
      "Epoch 11_b\n",
      "53/53 [==============================] - 173s 3s/step - loss: 0.3680 - tr_out_o_2_loss: 0.0022 - tr_out_q_2_loss: 7.4487e-04 - tr_out_p_2_loss: 0.3650 - tr_out_o_2_mean_squared_error: 0.0022 - tr_out_q_2_mean_squared_error: 7.4487e-04 - tr_out_p_2_accuracy: 0.8911\n",
      "Epoch 12_a\n",
      "77/77 [==============================] - 248s 3s/step - loss: 0.3210 - tr_out_o_loss: 0.0021 - tr_out_q_loss: 8.9146e-04 - tr_out_p_loss: 0.3180 - tr_out_o_mean_squared_error: 0.0021 - tr_out_q_mean_squared_error: 8.9146e-04 - tr_out_p_accuracy: 0.9031\n",
      "Epoch 12_b\n",
      "53/53 [==============================] - 169s 3s/step - loss: 0.3588 - tr_out_o_2_loss: 0.0022 - tr_out_q_2_loss: 7.4681e-04 - tr_out_p_2_loss: 0.3558 - tr_out_o_2_mean_squared_error: 0.0022 - tr_out_q_2_mean_squared_error: 7.4681e-04 - tr_out_p_2_accuracy: 0.8936\n",
      "Epoch 13_a\n",
      "68/77 [=========================>....] - ETA: 29s - loss: 0.3016 - tr_out_o_loss: 0.0020 - tr_out_q_loss: 9.4720e-04 - tr_out_p_loss: 0.2986 - tr_out_o_mean_squared_error: 0.0020 - tr_out_q_mean_squared_error: 9.4720e-04 - tr_out_p_accuracy: 0.9094"
     ]
    }
   ],
   "source": [
    "# train the two models with alternating epochs\n",
    "epochs_c = 80\n",
    "for i in range(epochs_c):\n",
    "    print (\"Epoch\", str(i+1)+\"_a\")\n",
    "    train.fit(generatorex(dataX1_o, dataX1_q, dataX1_p, seq_length, batch_size=5400), steps_per_epoch= (dtlngth[0]-split_i[0]) // 5400)\n",
    "    print (\"Epoch\", str(i+1)+\"_b\")\n",
    "    train_2.fit(generatorex(dataX1_o_2, dataX1_q_2, dataX1_p_2, seq_length, batch_size=5400), steps_per_epoch= (dtlngth[1]-split_i[1]) // 5400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save(\"train.h5\")\n",
    "train_2.save(\"train_2.h5\")\n",
    "infenc.save(\"infencb.h5\")\n",
    "infdec.save(\"infdecb.h5\")\n",
    "infdec_2.save(\"infdec_2b.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
