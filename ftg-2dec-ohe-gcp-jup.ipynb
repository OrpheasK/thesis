{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:32.389005Z",
     "iopub.status.busy": "2021-02-17T12:29:32.388387Z",
     "iopub.status.idle": "2021-02-17T12:29:39.152529Z",
     "shell.execute_reply": "2021-02-17T12:29:39.151571Z"
    },
    "papermill": {
     "duration": 6.778333,
     "end_time": "2021-02-17T12:29:39.152762",
     "exception": false,
     "start_time": "2021-02-17T12:29:32.374429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:32.389005Z",
     "iopub.status.busy": "2021-02-17T12:29:32.388387Z",
     "iopub.status.idle": "2021-02-17T12:29:39.152529Z",
     "shell.execute_reply": "2021-02-17T12:29:39.151571Z"
    },
    "papermill": {
     "duration": 6.778333,
     "end_time": "2021-02-17T12:29:39.152762",
     "exception": false,
     "start_time": "2021-02-17T12:29:32.374429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from math import trunc\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils, plot_model, to_categorical\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:39.193368Z",
     "iopub.status.busy": "2021-02-17T12:29:39.175650Z",
     "iopub.status.idle": "2021-02-17T12:29:39.202305Z",
     "shell.execute_reply": "2021-02-17T12:29:39.201781Z"
    },
    "papermill": {
     "duration": 0.042365,
     "end_time": "2021-02-17T12:29:39.202471",
     "exception": false,
     "start_time": "2021-02-17T12:29:39.160106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(dim_o, dim_q, dim_p, n_units):\n",
    "\t# define training encoder\n",
    "\tenc_in_o = Input(shape=(None, dim_o))\n",
    "\tenc_in_q = Input(shape=(None, dim_q))\n",
    "\tenc_in_p = Input(shape=(None, dim_p))\n",
    "\tencoder_inputs = concatenate([enc_in_o, enc_in_q, enc_in_p])\n",
    "\tencoder = LSTM(n_units, return_state=True)\n",
    "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\tencoder_states = [state_h, state_c]\n",
    "\t\n",
    "\t# define training decoder 1\n",
    "\tdec_in_o = Input(shape=(None, dim_o))\n",
    "\tdec_in_q = Input(shape=(None, dim_q))\n",
    "\tdec_in_p = Input(shape=(None, dim_p))\n",
    "\tdecoder_inputs = concatenate([dec_in_o, dec_in_q, dec_in_p])\n",
    "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\tdec_dense_o = Dense(dim_o, activation='softmax', name='tr_out_o')\n",
    "\tdec_dense_q = Dense(dim_q, activation='softmax', name='tr_out_q')\n",
    "\tdec_dense_p = Dense(dim_p, activation='softmax', name='tr_out_p')\n",
    "\t#out_o = Dense(1, activation='relu', name='tr_out_o')(decoder_outputs)#act relu\n",
    "\t#out_q = Dense(1, activation='sigmoid', name='tr_out_q')(decoder_outputs)\n",
    "\t#out_p = Dense(dim_p, activation='softmax', name='tr_out_p')(decoder_outputs)\n",
    "\tout_o = dec_dense_o(decoder_outputs)\n",
    "\tout_q = dec_dense_q(decoder_outputs)\n",
    "\tout_p = dec_dense_p(decoder_outputs)\n",
    "\n",
    "\t# define training decoder 2\n",
    "\tdec_in_o_2 = Input(shape=(None, dim_o))\n",
    "\tdec_in_q_2 = Input(shape=(None, dim_q))\n",
    "\tdec_in_p_2 = Input(shape=(None, dim_p))\n",
    "\tdecoder_inputs_2 = concatenate([dec_in_o_2, dec_in_q_2, dec_in_p_2])\n",
    "\tdecoder_lstm_2 = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs_2, _, _ = decoder_lstm_2(decoder_inputs_2, initial_state=encoder_states)\n",
    "\tdec_dense_o_2 = Dense(dim_o, activation='softmax', name='tr_out_o_2')\n",
    "\tdec_dense_q_2 = Dense(dim_q, activation='softmax', name='tr_out_q_2')\n",
    "\tdec_dense_p_2 = Dense(dim_p, activation='softmax', name='tr_out_p_2')\n",
    "\t#out_o = Dense(1, activation='relu', name='tr_out_o')(decoder_outputs)#act relu\n",
    "\t#out_q = Dense(1, activation='sigmoid', name='tr_out_q')(decoder_outputs)\n",
    "\t#out_p = Dense(dim_p, activation='softmax', name='tr_out_p')(decoder_outputs)\n",
    "\tout_o_2 = dec_dense_o_2(decoder_outputs_2)\n",
    "\tout_q_2 = dec_dense_q_2(decoder_outputs_2)\n",
    "\tout_p_2 = dec_dense_p_2(decoder_outputs_2)\n",
    "\t\n",
    "\tmodel = Model([enc_in_o, enc_in_q, enc_in_p, dec_in_o, dec_in_q, dec_in_p], [out_o, out_q, out_p])\n",
    "\tmodel_2 = Model([enc_in_o, enc_in_q, enc_in_p, dec_in_o_2, dec_in_q_2, dec_in_p_2], [out_o_2, out_q_2, out_p_2])\n",
    "\t\n",
    "\t# define inference encoder\n",
    "\tencoder_model = Model([enc_in_o, enc_in_q, enc_in_p], encoder_states)\n",
    "\t\n",
    "\t# define inference decoder 1\n",
    "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\tdecoder_states = [state_h, state_c]\n",
    "\t#out_o = TimeDistributed(Dense(1, activation='relu'))(decoder_outputs)#act relu\n",
    "\t#out_q = TimeDistributed(Dense(1, activation='sigmoid'))(decoder_outputs)\n",
    "\t#out_p = TimeDistributed(Dense(dim_p, activation='softmax'))(decoder_outputs)\n",
    "\tout_o = dec_dense_o(decoder_outputs)\n",
    "\tout_q = dec_dense_q(decoder_outputs)\n",
    "\tout_p = dec_dense_p(decoder_outputs)\n",
    "\n",
    "\t# define inference decoder 2\n",
    "\tdecoder_state_input_h_2 = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c_2 = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs_2 = [decoder_state_input_h, decoder_state_input_c_2]\n",
    "\tdecoder_outputs_2, state_h_2, state_c_2 = decoder_lstm_2(decoder_inputs_2, initial_state=decoder_states_inputs_2)\n",
    "\tdecoder_states_2 = [state_h_2, state_c_2]\n",
    "\t#out_o = TimeDistributed(Dense(1, activation='relu'))(decoder_outputs)#act relu\n",
    "\t#out_q = TimeDistributed(Dense(1, activation='sigmoid'))(decoder_outputs)\n",
    "\t#out_p = TimeDistributed(Dense(dim_p, activation='softmax'))(decoder_outputs)\n",
    "\tout_o_2 = dec_dense_o_2(decoder_outputs_2)\n",
    "\tout_q_2 = dec_dense_q_2(decoder_outputs_2)\n",
    "\tout_p_2 = dec_dense_p_2(decoder_outputs_2)\n",
    "\n",
    "\tdecoder_model = Model([dec_in_o, dec_in_q, dec_in_p] + decoder_states_inputs, [out_o, out_q, out_p] + decoder_states)\n",
    "\tdecoder_model_2 = Model([dec_in_o_2, dec_in_q_2, dec_in_p_2] + decoder_states_inputs_2, [out_o_2, out_q_2, out_p_2] + decoder_states_2)\n",
    "\n",
    "\t# return all models\n",
    "\treturn model, model_2, encoder_model, decoder_model, decoder_model_2\n",
    "\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, src_o, src_q, src_p, n_steps, cardinality_o, cardinality_q, cardinality_p):\n",
    "\t# encode\n",
    "\tstate = infenc.predict([src_o, src_q, src_p])\n",
    "\t# start of sequence input\n",
    "\ttarget_o = np.array([0.0 for _ in range(cardinality_o)]).reshape(1, 1, cardinality_o)\n",
    "\ttarget_q = np.array([0.0 for _ in range(cardinality_q)]).reshape(1, 1, cardinality_q)\n",
    "\t#target_p = 0\n",
    "\ttarget_p = np.array([0.0 for _ in range(cardinality_p)]).reshape(1, 1, cardinality_p)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\t#print(target_o.shape)\n",
    "\t\t#print(target_q.shape)\n",
    "\t\t#print(target_p.shape)\n",
    "\t\t#print(state[0].shape)\n",
    "\t\to, q, p, h, c = infdec.predict([target_o, target_q, target_p] + state)\n",
    "\t\t#print(a)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(o[0,0,:])\n",
    "\t\toutput.append(q[0,0,:])\n",
    "\t\toutput.append(p[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_o = o\n",
    "\t\ttarget_q = q\n",
    "\t\ttarget_p = p\n",
    "\treturn output\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [np.argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "#create list with window length sequences of list a data\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "#create directory for offset, quarterlength one hot encoding\n",
    "def qldir(m_o, qi):\n",
    "    fullql = []\n",
    "    for i in range(0, int(m_o), 1):\n",
    "        #j = str(i)   \n",
    "        fullql.append(i+.0) \n",
    "        fullql.append(i+.25)\n",
    "        fullql.append(i+.33)\n",
    "        fullql.append(i+.5)\n",
    "        fullql.append(i+.66)\n",
    "        fullql.append(i+.75)\n",
    "    fullql.append(m_o)\n",
    "    fullql.append(m_o+.25)\n",
    "    fullql[10] = 1.66\n",
    "    \n",
    "    if (qi == True):\n",
    "        r_dict = dict((c, i) for i, c in enumerate(fullql))\n",
    "    else:\n",
    "        r_dict = dict((i, c) for i, c in enumerate(fullql))\n",
    "    \n",
    "    return r_dict\n",
    "\n",
    "#generate train inputs and outputs while one hot encoding pitch and padding for seq2seq\n",
    "def generatorex(features1, features2, features3, seq_length, ft_o, ft_q, ft_p, m_o, m_q, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels# \n",
    "    q_to_i = qldir(m_o, True)\n",
    "    batch_features1 = np.zeros((batch_size, seq_length, ft_o))\n",
    "    batch_features2 = np.zeros((batch_size, seq_length, ft_q))\n",
    "    batch_features3 = np.zeros((batch_size, seq_length, ft_p))\n",
    "    batch_feat_pad1 = np.zeros((batch_size, seq_length, ft_o))\n",
    "    batch_feat_pad2 = np.zeros((batch_size, seq_length, ft_q))\n",
    "    batch_feat_pad3 = np.zeros((batch_size, seq_length, ft_p))\n",
    "    i = 0\n",
    "    while True:\n",
    "        for b in range(batch_size):\n",
    "            batch_features1[b] = to_categorical([q_to_i[ql[0]] for ql in features1[i]], num_classes=ft_o)\n",
    "            batch_features2[b] = to_categorical([q_to_i[ql[0]] for ql in features2[i]], num_classes=ft_q)\n",
    "            batch_features3[b] = to_categorical(features3[i], num_classes=ft_p)\n",
    "            batch_feat_pad1[b] = to_categorical(np.append([m_o+.25], [q_to_i[ql[0]] for ql in features1[i][:-1]]).reshape(seq_length, 1), num_classes=ft_o)\n",
    "            batch_feat_pad2[b] = to_categorical(np.append([m_q+.25], [q_to_i[ql[0]] for ql in features2[i][:-1]]).reshape(seq_length, 1), num_classes=ft_q)\n",
    "            batch_feat_pad3[b] = to_categorical(np.append([0], features3[i][:-1]).reshape(seq_length, 1), num_classes=ft_p)\n",
    "            i += 1\n",
    "            if (i == len(features1)):\n",
    "                i=0\n",
    "        #print(batch_features, batch_labels)\n",
    "        yield [batch_features1, batch_features2, batch_features3, batch_feat_pad1, batch_feat_pad2, batch_feat_pad3], [batch_features1, batch_features2, batch_features3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:39.224352Z",
     "iopub.status.busy": "2021-02-17T12:29:39.223732Z",
     "iopub.status.idle": "2021-02-17T12:29:46.116437Z",
     "shell.execute_reply": "2021-02-17T12:29:46.115777Z"
    },
    "papermill": {
     "duration": 6.907131,
     "end_time": "2021-02-17T12:29:46.116595",
     "exception": false,
     "start_time": "2021-02-17T12:29:39.209464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "stream_list = []\n",
    "stream_list_2 = []\n",
    "\n",
    "for path, subdirectories, files in os.walk('input/data-rock'):\n",
    "    for name in files:\n",
    "        with open(os.path.join(path, name), 'r') as f: \n",
    "            reader = csv.reader(f)\n",
    "            sub_list = [list(map(float,rec)) for rec in csv.reader(f, delimiter=',')]\n",
    "            stream_list = stream_list + sub_list\n",
    "            \n",
    "for path, subdirectories, files in os.walk('input/data-jazz'):\n",
    "    for name in files:\n",
    "        with open(os.path.join(path, name), 'r') as f: \n",
    "            reader = csv.reader(f)\n",
    "            sub_list = [list(map(float,rec)) for rec in csv.reader(f, delimiter=',')]\n",
    "            stream_list_2 = stream_list_2 + sub_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:46.183165Z",
     "iopub.status.busy": "2021-02-17T12:29:46.151842Z",
     "iopub.status.idle": "2021-02-17T12:29:46.969250Z",
     "shell.execute_reply": "2021-02-17T12:29:46.968719Z"
    },
    "papermill": {
     "duration": 0.84569,
     "end_time": "2021-02-17T12:29:46.969406",
     "exception": false,
     "start_time": "2021-02-17T12:29:46.123716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create seperate data structures for each variable (offset, quarterlength, pitch)\n",
    "#normalise offset and quarterlength\n",
    "offs = []\n",
    "qlngth = []\n",
    "ptch = []\n",
    "\n",
    "offs_2 = []\n",
    "qlngth_2 = []\n",
    "ptch_2 = []\n",
    "\n",
    "max_o = 600.0\n",
    "max_q = 50.0\n",
    "\n",
    "offsb = max(element[0] for element in stream_list if element[0]<=max_o)\n",
    "qlngthb = max(element[1] for element in stream_list if element[1]<=max_q)\n",
    "#ptchb = 127.0\n",
    "offsb_2 = max(element[0] for element in stream_list_2 if element[0]<=max_o)\n",
    "qlngthb_2 = max(element[1] for element in stream_list_2 if element[1]<=max_q)\n",
    "\n",
    "for row in stream_list:\n",
    "    if (row[0] <= max_o and row[1] <= max_q):\n",
    "        offs.append(trunc(row[0]*100)/100)\n",
    "        qlngth.append(trunc(row[1]*100)/100)\n",
    "        ptch.append(row[2])\n",
    "        \n",
    "for row in stream_list_2:\n",
    "    if (row[0] <= max_o and row[1] <= max_q):\n",
    "        offs_2.append(trunc(row[0]*100)/100)\n",
    "        qlngth_2.append(trunc(row[1]*100)/100)\n",
    "        ptch_2.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:47.082936Z",
     "iopub.status.busy": "2021-02-17T12:29:47.012542Z",
     "iopub.status.idle": "2021-02-17T12:29:47.134803Z",
     "shell.execute_reply": "2021-02-17T12:29:47.133806Z"
    },
    "papermill": {
     "duration": 0.158205,
     "end_time": "2021-02-17T12:29:47.135064",
     "exception": false,
     "start_time": "2021-02-17T12:29:46.976859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  [465352, 322353]\n"
     ]
    }
   ],
   "source": [
    "#divide the sets in sequences of specific length \n",
    "dtlngth=[len(offs), len(offs_2)]\n",
    "n_features_o = int(max_o)*6+2\n",
    "n_features_q = int(max_q)*6+2\n",
    "n_features_p = 127+1\n",
    "seq_length = 20#100 groups of 3\n",
    "\n",
    "dataX1_o = rolling_window(np.asarray(offs), seq_length)\n",
    "dataX1_q = rolling_window(np.asarray(qlngth), seq_length)\n",
    "dataX1_p = rolling_window(np.asarray(ptch), seq_length)\n",
    "\n",
    "dataX1_o_2 = rolling_window(np.asarray(offs_2), seq_length)\n",
    "dataX1_q_2 = rolling_window(np.asarray(qlngth_2), seq_length)\n",
    "dataX1_p_2 = rolling_window(np.asarray(ptch_2), seq_length)\n",
    "\n",
    "n_patterns = [len(dataX1_p), len(dataX1_p_2)]\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:47.173781Z",
     "iopub.status.busy": "2021-02-17T12:29:47.172875Z",
     "iopub.status.idle": "2021-02-17T12:29:47.175973Z",
     "shell.execute_reply": "2021-02-17T12:29:47.176764Z"
    },
    "papermill": {
     "duration": 0.027464,
     "end_time": "2021-02-17T12:29:47.176995",
     "exception": false,
     "start_time": "2021-02-17T12:29:47.149531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reshape inputs to be [samples, time steps, features]\n",
    "dataX1_o = np.reshape(dataX1_o, (dtlngth[0] - seq_length + 1, seq_length, 1))\n",
    "dataX1_q = np.reshape(dataX1_q, (dtlngth[0] - seq_length + 1, seq_length, 1))\n",
    "dataX1_p = np.reshape(dataX1_p, (dtlngth[0] - seq_length + 1, seq_length, 1))\n",
    "\n",
    "dataX1_o_2 = np.reshape(dataX1_o_2, (dtlngth[1] - seq_length + 1, seq_length, 1))\n",
    "dataX1_q_2 = np.reshape(dataX1_q_2, (dtlngth[1] - seq_length + 1, seq_length, 1))\n",
    "dataX1_p_2 = np.reshape(dataX1_p_2, (dtlngth[1] - seq_length + 1, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:47.203346Z",
     "iopub.status.busy": "2021-02-17T12:29:47.202291Z",
     "iopub.status.idle": "2021-02-17T12:29:47.207482Z",
     "shell.execute_reply": "2021-02-17T12:29:47.207896Z"
    },
    "papermill": {
     "duration": 0.022488,
     "end_time": "2021-02-17T12:29:47.208143",
     "exception": false,
     "start_time": "2021-02-17T12:29:47.185655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Patterns:  [46535, 32235]\n"
     ]
    }
   ],
   "source": [
    "#divide data in train and validation sets\n",
    "split_i = [n_patterns[0]*10 // 100, n_patterns[1]*10 // 100]\n",
    "\n",
    "dataX1_o_v = dataX1_o[-split_i[0]:]\n",
    "dataX1_o = dataX1_o[:-split_i[0]]\n",
    "\n",
    "dataX1_q_v = dataX1_q[-split_i[0]:]\n",
    "dataX1_q = dataX1_q[:-split_i[0]]\n",
    "\n",
    "dataX1_p_v = dataX1_p[-split_i[0]:]\n",
    "dataX1_p = dataX1_p[:-split_i[0]]\n",
    "\n",
    "dataX1_o_v_2 = dataX1_o_2[-split_i[1]:]\n",
    "dataX1_o_2 = dataX1_o_2[:-split_i[1]]\n",
    "\n",
    "dataX1_q_v_2 = dataX1_q_2[-split_i[1]:]\n",
    "dataX1_q_2 = dataX1_q_2[:-split_i[1]]\n",
    "\n",
    "dataX1_p_v_2 = dataX1_p_2[-split_i[1]:]\n",
    "dataX1_p_2 = dataX1_p_2[:-split_i[1]]\n",
    "\n",
    "print (\"Validation Patterns: \", split_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:47.234182Z",
     "iopub.status.busy": "2021-02-17T12:29:47.233524Z",
     "iopub.status.idle": "2021-02-17T12:29:48.819944Z",
     "shell.execute_reply": "2021-02-17T12:29:48.819163Z"
    },
    "papermill": {
     "duration": 1.603714,
     "end_time": "2021-02-17T12:29:48.820118",
     "exception": false,
     "start_time": "2021-02-17T12:29:47.216404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure problem\n",
    "n_steps_out = seq_length\n",
    "# define models\n",
    "train, train_2, infenc, infdec, infdec_2 = define_models(n_features_o, n_features_q, n_features_p, 128)\n",
    "train.compile(optimizer='adam', loss={'tr_out_o': 'categorical_crossentropy', 'tr_out_q': 'categorical_crossentropy', 'tr_out_p': 'categorical_crossentropy'},\n",
    " metrics={'tr_out_o': 'accuracy', 'tr_out_q': 'accuracy', 'tr_out_p': 'accuracy'})\n",
    "train_2.compile(optimizer='adam', loss={'tr_out_o_2': 'categorical_crossentropy', 'tr_out_q_2': 'categorical_crossentropy', 'tr_out_p_2': 'categorical_crossentropy'},\n",
    " metrics={'tr_out_o_2': 'accuracy', 'tr_out_q_2': 'accuracy', 'tr_out_p_2': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T12:29:48.843588Z",
     "iopub.status.busy": "2021-02-17T12:29:48.842849Z",
     "iopub.status.idle": "2021-02-17T14:04:33.905557Z",
     "shell.execute_reply": "2021-02-17T14:04:33.905968Z"
    },
    "papermill": {
     "duration": 5685.077951,
     "end_time": "2021-02-17T14:04:33.906164",
     "exception": false,
     "start_time": "2021-02-17T12:29:48.828213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the two models with alternating epochs\n",
    "epochs_c = 10\n",
    "for i in range(epochs_c):\n",
    "    print (\"Epoch\", str(i+1)+\"_a\")\n",
    "    history1 = train.fit(generatorex(dataX1_o, dataX1_q, dataX1_p, seq_length, n_features_o, n_features_q, n_features_p, max_o, max_q, batch_size=540), steps_per_epoch= (dtlngth[0]-split_i[0]) // 540, verbose=2)\n",
    "    print (\"Epoch\", str(i+1)+\"_b\")\n",
    "    history2 = train_2.fit(generatorex(dataX1_o_2, dataX1_q_2, dataX1_p_2, seq_length, n_features_o, n_features_q, n_features_p, max_o, max_q, batch_size=540), steps_per_epoch= (dtlngth[1]-split_i[1]) // 540, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save(\"train.h5\")\n",
    "train_2.save(\"train_2.h5\")\n",
    "infenc.save(\"infencb.h5\")\n",
    "infdec.save(\"infdecb.h5\")\n",
    "infdec_2.save(\"infdec_2b.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save:\n",
    "f1 = open('history1.pckl', 'wb')\n",
    "pickle.dump(history1.history, f1)\n",
    "f1.close()\n",
    "\n",
    "f2 = open('history2.pckl', 'wb')\n",
    "pickle.dump(history1.history, f2)\n",
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-02-12-ubuntu-1804-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-02-12-ubuntu-1804-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5730.781179,
   "end_time": "2021-02-17T14:04:57.426904",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-17T12:29:26.645725",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
